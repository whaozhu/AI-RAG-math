{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221d9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenhao/projects/Hilbert scheme/math_env/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader,TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings, DashScopeEmbeddings\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from dashscope.rerank.text_rerank import TextReRank\n",
    "from operator import itemgetter\n",
    "from pylatexenc.latexwalker import LatexWalker, LatexEnvironmentNode\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from rank_bm25 import BM25Okapi\n",
    "# import jieba  # 中文分词（如处理中文）\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43f31804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f054a",
   "metadata": {},
   "source": [
    "读取环境中的api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c2e775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "os.environ[\"DASHSCOPE_API_KEY\"] = \"sk-55e981aa134b4c93a5a616d71f1ed301\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac23e264",
   "metadata": {},
   "source": [
    "基于 LaTeX 感知的智能分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85c38a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.documents import Document\n",
    "\n",
    "# # === LaTeX 感知的智能分割函数 ===\n",
    "# def latex_aware_split_documents(documents, chunk_size=800, chunk_overlap=50, preserve_envs=None):\n",
    "#     if preserve_envs is None:\n",
    "#         preserve_envs = {\n",
    "#             'theorem', 'lemma', 'definition', 'proof',\n",
    "#             'corollary', 'example', 'proposition', 'remark'\n",
    "#         }\n",
    "\n",
    "#     all_chunks = []\n",
    "\n",
    "#     for doc in documents:\n",
    "#         text = doc.page_content\n",
    "#         metadata = doc.metadata\n",
    "\n",
    "#         # 尝试用 pylatexenc 解析 LaTeX\n",
    "#         try:\n",
    "#             lw = LatexWalker(text)\n",
    "#             nodelist, _, _ = lw.get_latex_nodes(pos=0)\n",
    "#         except Exception as e:\n",
    "#             print(f\" LaTeX parsing failed for {metadata.get('source', 'unknown')}, falling back to plain split: {e}\")\n",
    "#             # 回退到普通文本分割\n",
    "#             fallback_splitter = RecursiveCharacterTextSplitter(\n",
    "#                 chunk_size=chunk_size,\n",
    "#                 chunk_overlap=chunk_overlap,\n",
    "#                 separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "#             )\n",
    "#             chunks = fallback_splitter.split_text(text)\n",
    "#             all_chunks.extend([\n",
    "#                 Document(page_content=chunk, metadata=metadata)\n",
    "#                 for chunk in chunks if chunk.strip()\n",
    "#             ])\n",
    "#             continue\n",
    "\n",
    "#         # 构建语义单元：普通文本累积，重要环境单独成块\n",
    "#         semantic_units = []\n",
    "#         current_text = \"\"\n",
    "\n",
    "#         for node in nodelist:\n",
    "#             if isinstance(node, LatexEnvironmentNode) and node.environmentname in preserve_envs:\n",
    "#                 if current_text.strip():\n",
    "#                     semantic_units.append((\"text\", current_text))\n",
    "#                     current_text = \"\"\n",
    "#                 env_str = node.latex_verbatim()\n",
    "#                 semantic_units.append((\"env\", env_str))\n",
    "#             else:\n",
    "#                 current_text += node.latex_verbatim()\n",
    "\n",
    "#         if current_text.strip():\n",
    "#             semantic_units.append((\"text\", current_text))\n",
    "\n",
    "#         # 合并为最终 chunks\n",
    "#         current_chunk = \"\"\n",
    "#         for unit_type, content in semantic_units:\n",
    "#             if unit_type == \"env\":\n",
    "#                 # 重要环境：单独成块（即使超限）\n",
    "#                 if current_chunk.strip():\n",
    "#                     all_chunks.append(Document(page_content=current_chunk, metadata=metadata))\n",
    "#                     current_chunk = \"\"\n",
    "#                 if content.strip():\n",
    "#                     all_chunks.append(Document(page_content=content, metadata=metadata))\n",
    "#             else:\n",
    "#                 # 普通文本：按 chunk_size 切分\n",
    "#                 text_splitter = RecursiveCharacterTextSplitter(\n",
    "#                     chunk_size=chunk_size,\n",
    "#                     chunk_overlap=chunk_overlap,\n",
    "#                     separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "#\n",
    "#                             \n",
    "#                                                               )\n",
    "\n",
    "\n",
    "\n",
    "#                 sub_chunks = text_splitter.split_text(content)\n",
    "#                 for sc in sub_chunks:\n",
    "#                     if len(current_chunk) + len(sc) <= chunk_size:\n",
    "#                         current_chunk += sc\n",
    "#                     else:\n",
    "#                         if current_chunk.strip():\n",
    "#                             all_chunks.append(Document(page_content=current_chunk, metadata=metadata))\n",
    "#                         current_chunk = sc\n",
    "#         if current_chunk.strip():\n",
    "#             all_chunks.append(Document(page_content=current_chunk, metadata=metadata))\n",
    "\n",
    "#     # 最终过滤空 chunk\n",
    "#     return [d for d in all_chunks if d.page_content.strip()]\n",
    "\n",
    "\n",
    "# # === 主流程 ===\n",
    "# loader_3 = TextLoader(file_path=file_path_3, encoding=\"utf-8\")\n",
    "# docs = loader_3.load()\n",
    "\n",
    "# # 使用 LaTeX 感知分割器（关键修改！）\n",
    "# splitte_docs = latex_aware_split_documents(\n",
    "#     docs,\n",
    "#     chunk_size=800,      # 建议 ≥800 以容纳完整定理\n",
    "#     chunk_overlap=50\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7e44db",
   "metadata": {},
   "source": [
    "外部知识文档地址，以及加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80c9963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只是用 loader_3 作为外部文档\n",
    "file_path_3=\"/home/wenhao/projects/Hilbert scheme/PDF/thesis-some chapter.txt\" \n",
    "loader_3 = TextLoader(file_path = file_path_3)\n",
    "docs = loader_3.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b58fd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63982/3591448706.py:30: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=persist_dir, embedding_function=embeddings)\n"
     ]
    }
   ],
   "source": [
    "# 基于txt 格式的文档分割\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\", \n",
    "                                r\"\\end{lemma}\",\n",
    "                                r\"\\end{theorem}\",\n",
    "                                r\"\\end{definition}\",\n",
    "                                r\"\\end{proof}\",\n",
    "                                r\"\\end{corollary}\",\n",
    "                                r\"\\end{example}\",\n",
    "                                r\"\\end{remark}\"\n",
    "                            ]\n",
    ")\n",
    "\n",
    "splitte_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "# 初始化 embedding 模型，指定 model=\"text-embedding-v4\"\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v4\"\n",
    ")\n",
    "\n",
    "\n",
    "# 向量数据库储存地址\n",
    "persist_dir = \"/home/wenhao/projects/Hilbert scheme/chroma_db\"\n",
    "\n",
    "# 向量数据库， 如果已存在，不需要重复创建\n",
    "if os.path.exists(persist_dir) and os.listdir(persist_dir):\n",
    "    db = Chroma(persist_directory=persist_dir, embedding_function=embeddings)\n",
    "else:\n",
    "    db = Chroma.from_documents(\n",
    "        documents=splitte_docs,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_dir\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7e97ec",
   "metadata": {},
   "source": [
    "混合检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78dd6e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 稠密搜索\n",
    "from collections import defaultdict\n",
    "retriever = db.as_retriever( search_kwargs={\"k\": 50} )\n",
    "\n",
    "\n",
    "# 离散搜索\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "\n",
    "tokenized_docs = [tokenize(doc.page_content) for doc in docs]\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "def bm25_retrieve(query, k):\n",
    "    tokenized_query = tokenize(query)\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_k_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
    "    return [docs[i] for i in top_k_idx]\n",
    "\n",
    "\n",
    "# 合并去重， RRF（Reciprocal Rank Fusion）\n",
    "def merge_results_rrf(vec_results, bm25_results, rrf_k=60):\n",
    "    \"\"\"\n",
    "    使用 Reciprocal Rank Fusion (RRF) 融合两个结果列表\n",
    "    \"\"\"\n",
    "    rrf_score = defaultdict(float)\n",
    "    doc_map = {}  # page_content -> Document（保留首次出现的 Document 对象）\n",
    "\n",
    "    # 稠密检索结果贡献 RRF 分数（排名从 0 开始）\n",
    "    for rank, doc in enumerate(vec_results):\n",
    "        content = doc.page_content\n",
    "        if content not in doc_map:\n",
    "            doc_map[content] = doc\n",
    "        rrf_score[content] += 1.0 / (rrf_k + rank + 1)\n",
    "\n",
    "    # 稀疏检索结果贡献 RRF 分数\n",
    "    for rank, doc in enumerate(bm25_results):\n",
    "        content = doc.page_content\n",
    "        if content not in doc_map:\n",
    "            doc_map[content] = doc\n",
    "        rrf_score[content] += 1.0 / (rrf_k + rank + 1)\n",
    "\n",
    "    # 按 RRF 分数降序排序\n",
    "    sorted_items = sorted(\n",
    "        doc_map.items(),\n",
    "        key=lambda x: rrf_score[x[0]],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    # 返回去重并按 RRF 排序的文档列表\n",
    "    return [doc for _, doc in sorted_items]\n",
    "\n",
    "\n",
    "def retriever_hybrid(query, top_k=20):\n",
    "    vec_results = retriever.invoke(query)          \n",
    "    bm25_results = bm25_retrieve(query, k=50)      \n",
    "    merged = merge_results_rrf(vec_results, bm25_results)\n",
    "    return merged[:top_k]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd6c250",
   "metadata": {},
   "source": [
    "Chunks 的 reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f5adf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reranker\n",
    "def rerank_retriever(query):\n",
    "    docs = retriever_hybrid(query)\n",
    "    clean_docs = [d for d in docs if d.page_content.strip()]\n",
    "    initial_docs = [d.page_content for d in clean_docs]\n",
    "\n",
    "    # print(f\"Number of input docs to reranker: {len(initial_docs)}\")\n",
    "\n",
    "    if not initial_docs:\n",
    "        return []\n",
    "\n",
    "    response = TextReRank.call(\n",
    "        model=\"qwen3-rerank\",\n",
    "        query=query,\n",
    "        documents=initial_docs,\n",
    "        top_n= 5\n",
    "        # instruct=\"Given a question, retrieve passages that best answer it.\"\n",
    "    )\n",
    "    \n",
    "    text_to_doc = {d.page_content: d for d in clean_docs}\n",
    "    \n",
    "    reranked_docs = []\n",
    "    for item in response.output.results:\n",
    "        idx = item.index  \n",
    "        if 0 <= idx < len(initial_docs):\n",
    "            text = initial_docs[idx]\n",
    "            if text in text_to_doc:\n",
    "                reranked_docs.append(text_to_doc[text])\n",
    "\n",
    "    return reranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fdd8d5",
   "metadata": {},
   "source": [
    "reranker 的调试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "153554b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is Punctual Hilbert scheme?\"\n",
    "# results = db.similarity_search_with_score(query, k=5)\n",
    "# for doc, score in results:\n",
    "#     print(f\"Score: {score:.4f}\")\n",
    "#     print(doc.page_content)\n",
    "#     print(\"-\" * 40)\n",
    "\n",
    "# top5_chunks = rerank_retriever(query)\n",
    "\n",
    "# # 打印结果\n",
    "# for i, doc in enumerate(top5_chunks, 1):\n",
    "#     print(f\"Top {i}:\")\n",
    "#     print(doc.page_content)\n",
    "#     print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d513d2b",
   "metadata": {},
   "source": [
    "多轮对话\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01057885",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  定义不同轮次的特定prompt\n",
    "def get_prompt_for_round(round_num):\n",
    "    \"\"\"根据对话轮次返回特定prompt\"\"\"\n",
    "    if round_num == 1:\n",
    "        return \"专注于介绍相关数学的基础知识。请用简单易懂的语言回答问题。\"\n",
    "    elif round_num == 2:\n",
    "        return \"专注于深入解释相关数学的概念。请提供更详细的技术细节。\"\n",
    "    elif round_num >= 3:\n",
    "        return \"，专注于解决复杂问题和提供专业见解。请提供深度分析。\"\n",
    "    return \"你是一个数学教授，根据上下文回答问题。\"\n",
    "\n",
    "\n",
    "# 4. 构建链式结构\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个严谨的数学专家。\\n\"\n",
    "    #  \"用户额外提供了以下参考文档，如何文档非空白内容，请结合该文档和检索结果回答：\\n{file}\\n\\n\"\n",
    "     \"检索到的相关上下文如下：\\n{context}\\n\\n\"\n",
    "     \"如果以上内容不包含答案，请回答：'根据已有资料无法回答。'\" \"{system_prompt}\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "# 初始化LLM\n",
    "llm = ChatTongyi(model = \"qwen-turbo\",temperature = 0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# 创建链\n",
    "chain = (\n",
    "{     \n",
    "    \"context\": itemgetter(\"question\") | RunnableLambda(rerank_retriever)| format_docs,\n",
    "    \"system_prompt\": RunnableLambda(itemgetter(\"round_num\")) | RunnableLambda(get_prompt_for_round),\n",
    "    \"history\": itemgetter(\"history\"),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "    # \"file\": itemgetter(\"file\")\n",
    "}\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# 创建对话历史管理器\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "conversational_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "#  运行多轮对话示例\n",
    "session_id = \"user_session_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ed73fd",
   "metadata": {},
   "source": [
    "用户query的重写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "038ee858",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_rewriting_query = ChatTongyi(\n",
    "    model=\"qwen3-1.7b\",\n",
    "    temperature=0,\n",
    "    model_kwargs={\"enable_thinking\": False}\n",
    ")\n",
    "\n",
    "\n",
    "prompt_rewrite = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"你是一个查询改写助手。请根据对话历史，将用户最新的问题改写成一个独立、完整、清晰的问题。\\n\"\n",
    "     \"不要回答问题，不要添加解释，只输出改写后的问题。\"\n",
    "    ),\n",
    "    (\"human\", \"对话历史：{history}\\n用户最新问题：{question}\")\n",
    "])\n",
    "\n",
    "\n",
    "def rewriting_query(question: str, history: list ) -> str:\n",
    "    chain = prompt_rewrite | llm_rewriting_query\n",
    "    history_str = \"\\n\".join([f\"{'用户' if isinstance(msg, HumanMessage) else 'AI'}: {msg.content}\" for msg in history])\n",
    "    response = chain.invoke({\"question\": question,\n",
    "                            \"history\": history_str})\n",
    "    return response.content.strip()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "983f0755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轮次1: content='Hilbert scheme 是数学中一个重要的概念，它用来“参数化”某些几何对象。具体来说，它是一个空间（可以理解为一种几何结构），其中的每一个点都对应着一个特定类型的子对象。\\n\\n举个简单的例子：假设你有一个几何图形，比如一条曲线，Hilbert scheme 就是用来“收集”所有在这个曲线上的有限子集（或者更一般的子结构）的集合。这些子集的大小（或者说“长度”）是固定的，比如都是由 5 个点组成的。\\n\\n在更抽象的数学语言中，Hilbert scheme 可以用来参数化某种代数结构，例如：\\n\\n- 在代数几何中，它参数化具有固定性质的子簇（subvarieties）；\\n- 在代数中，它可能参数化具有固定“余长”（colength）的理想（ideal）。\\n\\n简单来说，**Hilbert scheme 是一个用来分类和研究特定几何或代数对象的工具**，它把这些对象“放在一起”，形成一个整体的结构，便于我们研究它们的性质和相互关系。' additional_kwargs={} response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': '57ae93a6-5b1a-4661-a8ad-53476a5e9a63', 'token_usage': {'input_tokens': 657, 'output_tokens': 234, 'prompt_tokens_details': {'cached_tokens': 0}, 'total_tokens': 891}} id='lc_run--019bfa96-38c9-7612-a15c-d8b1025ded38-0' tool_calls=[] invalid_tool_calls=[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Hilbert scheme 是什么？\"\n",
    "rewritten_query = rewriting_query(query, get_session_history(session_id).messages)\n",
    "\n",
    "# loader_2 = TextLoader(file_path = file_path_2)\n",
    "# full_docs = loader_2.load()                         # 返回 Document 对象列表\n",
    "\n",
    "# # 合并所有页面文本\n",
    "# pdf_text = \"\\n\".join([page.page_content for page in full_docs])\n",
    "\n",
    "# 第一轮对话\n",
    "response = conversational_chain.invoke(\n",
    "    {     \n",
    "        #  \"file\": pdf_text, \n",
    "        \"question\": rewritten_query,\n",
    "        \"round_num\": 1,\n",
    "           \n",
    "    },\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(f\"轮次1: {response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math_env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221d9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader,TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from dashscope.rerank.text_rerank import TextReRank\n",
    "from operator import itemgetter\n",
    "from pylatexenc.latexwalker import LatexWalker, LatexEnvironmentNode\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f054a",
   "metadata": {},
   "source": [
    "读取环境中的api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "os.environ[\"DASHSCOPE_API_KEY\"] = \"your_dashcope_api_key\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac23e264",
   "metadata": {},
   "source": [
    "基于 LaTeX 感知的智能分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c38a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.documents import Document\n",
    "\n",
    "# # === LaTeX 感知的智能分割函数 ===\n",
    "# def latex_aware_split_documents(documents, chunk_size=800, chunk_overlap=50, preserve_envs=None):\n",
    "#     if preserve_envs is None:\n",
    "#         preserve_envs = {\n",
    "#             'theorem', 'lemma', 'definition', 'proof',\n",
    "#             'corollary', 'example', 'proposition', 'remark'\n",
    "#         }\n",
    "\n",
    "#     all_chunks = []\n",
    "\n",
    "#     for doc in documents:\n",
    "#         text = doc.page_content\n",
    "#         metadata = doc.metadata\n",
    "\n",
    "#         # 尝试用 pylatexenc 解析 LaTeX\n",
    "#         try:\n",
    "#             lw = LatexWalker(text)\n",
    "#             nodelist, _, _ = lw.get_latex_nodes(pos=0)\n",
    "#         except Exception as e:\n",
    "#             print(f\"⚠️ LaTeX parsing failed for {metadata.get('source', 'unknown')}, falling back to plain split: {e}\")\n",
    "#             # 回退到普通文本分割\n",
    "#             fallback_splitter = RecursiveCharacterTextSplitter(\n",
    "#                 chunk_size=chunk_size,\n",
    "#                 chunk_overlap=chunk_overlap,\n",
    "#                 separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "#             )\n",
    "#             chunks = fallback_splitter.split_text(text)\n",
    "#             all_chunks.extend([\n",
    "#                 Document(page_content=chunk, metadata=metadata)\n",
    "#                 for chunk in chunks if chunk.strip()\n",
    "#             ])\n",
    "#             continue\n",
    "\n",
    "#         # 构建语义单元：普通文本累积，重要环境单独成块\n",
    "#         semantic_units = []\n",
    "#         current_text = \"\"\n",
    "\n",
    "#         for node in nodelist:\n",
    "#             if isinstance(node, LatexEnvironmentNode) and node.environmentname in preserve_envs:\n",
    "#                 if current_text.strip():\n",
    "#                     semantic_units.append((\"text\", current_text))\n",
    "#                     current_text = \"\"\n",
    "#                 env_str = node.latex_verbatim()\n",
    "#                 semantic_units.append((\"env\", env_str))\n",
    "#             else:\n",
    "#                 current_text += node.latex_verbatim()\n",
    "\n",
    "#         if current_text.strip():\n",
    "#             semantic_units.append((\"text\", current_text))\n",
    "\n",
    "#         # 合并为最终 chunks\n",
    "#         current_chunk = \"\"\n",
    "#         for unit_type, content in semantic_units:\n",
    "#             if unit_type == \"env\":\n",
    "#                 # 重要环境：单独成块（即使超限）\n",
    "#                 if current_chunk.strip():\n",
    "#                     all_chunks.append(Document(page_content=current_chunk, metadata=metadata))\n",
    "#                     current_chunk = \"\"\n",
    "#                 if content.strip():\n",
    "#                     all_chunks.append(Document(page_content=content, metadata=metadata))\n",
    "#             else:\n",
    "#                 # 普通文本：按 chunk_size 切分\n",
    "#                 text_splitter = RecursiveCharacterTextSplitter(\n",
    "#                     chunk_size=chunk_size,\n",
    "#                     chunk_overlap=chunk_overlap,\n",
    "#                     separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "#\n",
    "#                             \n",
    "#                                                               )\n",
    "\n",
    "\n",
    "\n",
    "#                 sub_chunks = text_splitter.split_text(content)\n",
    "#                 for sc in sub_chunks:\n",
    "#                     if len(current_chunk) + len(sc) <= chunk_size:\n",
    "#                         current_chunk += sc\n",
    "#                     else:\n",
    "#                         if current_chunk.strip():\n",
    "#                             all_chunks.append(Document(page_content=current_chunk, metadata=metadata))\n",
    "#                         current_chunk = sc\n",
    "#         if current_chunk.strip():\n",
    "#             all_chunks.append(Document(page_content=current_chunk, metadata=metadata))\n",
    "\n",
    "#     # 最终过滤空 chunk\n",
    "#     return [d for d in all_chunks if d.page_content.strip()]\n",
    "\n",
    "\n",
    "# # === 主流程 ===\n",
    "# loader_3 = TextLoader(file_path=file_path_3, encoding=\"utf-8\")\n",
    "# docs = loader_3.load()\n",
    "\n",
    "# # 使用 LaTeX 感知分割器（关键修改！）\n",
    "# splitte_docs = latex_aware_split_documents(\n",
    "#     docs,\n",
    "#     chunk_size=800,      # 建议 ≥800 以容纳完整定理\n",
    "#     chunk_overlap=50\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7e44db",
   "metadata": {},
   "source": [
    "外部知识文档地址，以及加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c9963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只是用 loader_3 作为外部文档\n",
    "file_path_3=\"/./thesis-some chapter.txt\" \n",
    "loader_3 = TextLoader(file_path = file_path_3)\n",
    "docs = loader_3.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于txt 格式的文档分割\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\", \n",
    "                                r\"\\end{lemma}\",\n",
    "                                r\"\\end{theorem}\",\n",
    "                                r\"\\end{definition}\",\n",
    "                                r\"\\end{proof}\",\n",
    "                                r\"\\end{corollary}\",\n",
    "                                r\"\\end{example}\",\n",
    "                                r\"\\end{remark}\"\n",
    "                            ]\n",
    ")\n",
    "\n",
    "splitte_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "# 初始化 embedding 模型，指定 model=\"text-embedding-v4\"\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v4\"\n",
    ")\n",
    "\n",
    "\n",
    "# 向量数据库储存地址\n",
    "persist_dir = \"/home/wenhao/projects/Hilbert scheme/chroma_db\"\n",
    "\n",
    "# 向量数据库， 如果已存在，不需要重复创建\n",
    "if os.path.exists(persist_dir) and os.listdir(persist_dir):\n",
    "    db = Chroma(persist_directory=persist_dir, embedding_function=embeddings)\n",
    "else:\n",
    "    db = Chroma.from_documents(\n",
    "        documents=splitte_docs,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_dir\n",
    "    )\n",
    "\n",
    "retriever = db.as_retriever( search_kwargs={\"k\": 20} )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd6c250",
   "metadata": {},
   "source": [
    "Chunks 的 reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5adf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reranker\n",
    "def rerank_retriever(query):\n",
    "    docs = retriever.invoke(query)\n",
    "    clean_docs = [d for d in docs if d.page_content.strip()]\n",
    "    initial_docs = [d.page_content for d in clean_docs]\n",
    "\n",
    "    # print(f\"Number of input docs to reranker: {len(initial_docs)}\")\n",
    "\n",
    "    if not initial_docs:\n",
    "        return []\n",
    "\n",
    "    response = TextReRank.call(\n",
    "        model=\"qwen3-rerank\",\n",
    "        query=query,\n",
    "        documents=initial_docs,\n",
    "        top_n= 5\n",
    "        # instruct=\"Given a question, retrieve passages that best answer it.\"\n",
    "    )\n",
    "    \n",
    "    text_to_doc = {d.page_content: d for d in clean_docs}\n",
    "    \n",
    "    reranked_docs = []\n",
    "    for item in response.output.results:\n",
    "        idx = item.index  # ✅ 正确方式：使用 index\n",
    "        if 0 <= idx < len(initial_docs):\n",
    "            text = initial_docs[idx]\n",
    "            if text in text_to_doc:\n",
    "                reranked_docs.append(text_to_doc[text])\n",
    "\n",
    "    return reranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fdd8d5",
   "metadata": {},
   "source": [
    "reranker 的调试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153554b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.3790\n",
      "\\section{Punctual Hilbert schemes}\\label{Punctual Hilbert schemes}\n",
      "----------------------------------------\n",
      "Score: 0.6119\n",
      "embedding. In general, even when the defining equations of punctual Hilbert schemes are known, it remains difficult to understand their geometry or to compute their invariants.\n",
      "----------------------------------------\n",
      "Score: 0.6227\n",
      "The main objects of study in this article are the punctual Hilbert schemes of irreducible curve singularities defined over an algebraically closed field  $ k $  of characteristic～ $ 0 $ . Given a curve singularity  $ (C,O) $  and an integer  $ \\ell \\in \\mathbb{N} $ , the  $ \\ell $ -th punctual Hilbert scheme of  $ (C,O) $ , denoted  $ C^{[\\ell]} $ , is the moduli space parametrizing  $ 0 $ -dimensional subschemes of  $ (C,O) $  of length  $ \\ell $ . This is a special case of Grothendieck's\n",
      "----------------------------------------\n",
      "Score: 0.6240\n",
      "Recently, punctual Hilbert schemes have attracted considerable attention from two distinct but interconnected perspectives, both of which provide key motivations for this work.\n",
      "On one hand, these schemes are the fundamental building blocks for the \\emph{motivic Hilbert zeta function}, introduced by Bejleri, Ranganathan and Vakil～\\cite{bejleri2020motivic}. This zeta function is defined as the generating series of the classes of punctual Hilbert schemes in the Grothendieck ring of varieties:\n",
      "----------------------------------------\n",
      "Score: 0.6811\n",
      "For curve singularities with a single Puiseux pair, Oblomkov, Rasmussen, and Shende proved that the punctual Hilbert scheme  $ C^{[\\ell]} $  admits a stratification by affine spaces, a result that plays a foundational role in the motivic study of these moduli spaces.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# query = \"What is Punctual Hilbert scheme?\"\n",
    "# results = db.similarity_search_with_score(query, k=5)\n",
    "# for doc, score in results:\n",
    "#     print(f\"Score: {score:.4f}\")\n",
    "#     print(doc.page_content)\n",
    "#     print(\"-\" * 40)\n",
    "\n",
    "# top5_chunks = rerank_retriever(query)\n",
    "\n",
    "# # 打印结果\n",
    "# for i, doc in enumerate(top5_chunks, 1):\n",
    "#     print(f\"Top {i}:\")\n",
    "#     print(doc.page_content)\n",
    "#     print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d513d2b",
   "metadata": {},
   "source": [
    "多轮对话\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01057885",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  定义不同轮次的特定prompt\n",
    "def get_prompt_for_round(round_num):\n",
    "    \"\"\"根据对话轮次返回特定prompt\"\"\"\n",
    "    if round_num == 1:\n",
    "        return \"专注于介绍相关数学的基础知识。请用简单易懂的语言回答问题。\"\n",
    "    elif round_num == 2:\n",
    "        return \"专注于深入解释相关数学的概念。请提供更详细的技术细节。\"\n",
    "    elif round_num >= 3:\n",
    "        return \"，专注于解决复杂问题和提供专业见解。请提供深度分析。\"\n",
    "    return \"你是一个数学教授，根据上下文回答问题。\"\n",
    "\n",
    "\n",
    "# 4. 构建链式结构\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个严谨的数学专家。\\n\"\n",
    "    #  \"用户额外提供了以下参考文档，如何文档非空白内容，请结合该文档和检索结果回答：\\n{file}\\n\\n\"\n",
    "     \"检索到的相关上下文如下：\\n{context}\\n\\n\"\n",
    "     \"如果以上内容不包含答案，请回答：'根据已有资料无法回答。'\" \"{system_prompt}\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "# 初始化LLM\n",
    "llm = ChatTongyi(model = \"qwen-turbo\",temperature = 0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# 创建链\n",
    "chain = (\n",
    "{     \n",
    "    \"context\": itemgetter(\"question\") | RunnableLambda(rerank_retriever)| format_docs,\n",
    "    \"system_prompt\": RunnableLambda(itemgetter(\"round_num\")) | RunnableLambda(get_prompt_for_round),\n",
    "    \"history\": itemgetter(\"history\"),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "    # \"file\": itemgetter(\"file\")\n",
    "}\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# 创建对话历史管理器\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "conversational_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "#  运行多轮对话示例\n",
    "session_id = \"user_session_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ed73fd",
   "metadata": {},
   "source": [
    "用户query的重写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ee858",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_rewriting_query = ChatTongyi(\n",
    "    model=\"qwen3-1.7b\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    model_kwargs={\"enable_thinking\": False}\n",
    ")\n",
    "\n",
    "\n",
    "prompt_rewrite = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"你是一个查询改写助手。请根据对话历史，将用户最新的问题改写成一个独立、完整、清晰的问题。\\n\"\n",
    "     \"不要回答问题，不要添加解释，只输出改写后的问题。\"\n",
    "    ),\n",
    "    (\"human\", \"对话历史：{history}\\n用户最新问题：{question}\")\n",
    "])\n",
    "\n",
    "\n",
    "def rewriting_query(question: str, history: list ) -> str:\n",
    "    chain = prompt_rewrite | llm_rewriting_query\n",
    "    history_str = \"\\n\".join([f\"{'用户' if isinstance(msg, HumanMessage) else 'AI'}: {msg.content}\" for msg in history])\n",
    "    response = chain.invoke({\"question\": question,\n",
    "                            \"history\": history_str})\n",
    "    return response.content.strip()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aa4ee8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "983f0755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轮次1: content='Hilbert scheme 是数学中一个重要的概念，主要用于研究几何对象（如曲线、曲面等）的子集的结构。\\n\\n简单来说，**Hilbert scheme** 是一个“空间”，它把具有某种特定性质的子集（比如长度为 $\\\\ell$ 的 0-维子集）全部“放在一起”，并按照某种方式分类。这个“空间”中的每一个点，对应一个特定的子集。\\n\\n举个例子：  \\n假设你有一个曲线 $C$，在它的某一点 $O$ 附近，你想研究所有“长度为 $\\\\ell$”的子集。这些子集可以看作是某些代数方程的解的集合。Hilbert scheme 就是把这些子集都收集起来，并给出它们的结构和性质。\\n\\n更具体地说，对于一个曲线奇点 $(C, O)$，它的 **$\\\\ell$-th punctual Hilbert scheme**（记作 $C^{[\\\\ell]}$）就是所有长度为 $\\\\ell$ 的 0-维子集的集合，这些子集都位于点 $O$ 附近。\\n\\nHilbert scheme 在代数几何中有广泛应用，特别是在研究几何对象的变形、对称性以及不变量等方面。' additional_kwargs={} response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': 'bfbf9490-d852-437b-b2a8-483924b9044d', 'token_usage': {'input_tokens': 543, 'output_tokens': 262, 'prompt_tokens_details': {'cached_tokens': 0}, 'total_tokens': 805}} id='lc_run--019bc97f-7b10-77c2-adfd-9c07e156f2b7-0' tool_calls=[] invalid_tool_calls=[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Hilbert scheme 是什么？\"\n",
    "rewritten_query = rewriting_query(query, get_session_history(session_id).messages)\n",
    "\n",
    "# loader_2 = TextLoader(file_path = file_path_2)\n",
    "# full_docs = loader_2.load()                         # 返回 Document 对象列表\n",
    "\n",
    "# # 合并所有页面文本\n",
    "# pdf_text = \"\\n\".join([page.page_content for page in full_docs])\n",
    "\n",
    "# 第一轮对话\n",
    "response = conversational_chain.invoke(\n",
    "    {     \n",
    "        #  \"file\": pdf_text, \n",
    "        \"question\": rewritten_query,\n",
    "        \"round_num\": 1,\n",
    "           \n",
    "    },\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(f\"轮次1: {response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math_env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
